{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\YuniN\\anaconda3\\envs\\Ujicobalagi\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["'D:/belajar/Data/Flower/training'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","train_dir = 'D:/belajar/Data/Flower/training'\n","train_dir"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["<keras.src.preprocessing.image.ImageDataGenerator at 0x1eeefa3f350>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# data augmnetasi and normalization\n","train_datagen = ImageDataGenerator(\n","    rescale = 1./225,\n","    rotation_range = 40,\n","    width_shift_range = 0.2,\n","    height_shift_range = 0.2,\n","    shear_range = 0.2,\n","    zoom_range = 0.2,\n","    horizontal_flip = True,\n","    fill_mode = 'nearest',\n","    validation_split = 0.2\n",")\n","train_datagen"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2198 images belonging to 5 classes.\n"]},{"data":{"text/plain":["<keras.src.preprocessing.image.DirectoryIterator at 0x1eef85a7050>"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# create data generator (train)\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    subset ='training',\n","    target_size=(150,150),\n","    batch_size=32,\n","    class_mode='categorical'\n",")\n","train_generator"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 548 images belonging to 5 classes.\n"]}],"source":["validation_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    subset = 'validation',\n","    target_size=(150,150),\n","    batch_size=32,\n","    class_mode='categorical'\n",")"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Dropout\n","\n","model = Sequential([\n","    Conv2D(32, (3, 3), activation='relu', input_shape=(150,150,3)),\n","    MaxPooling2D((2, 2)),\n","    Conv2D(64, (3,3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Conv2D(128, (3,3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dropout(0.5),\n","    Dense(train_generator.num_classes, activation='softmax') # output layar with softmax activation\n","])"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping\n","early_stopping = EarlyStopping(monitor ='loss', patience=3)\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","68/68 [==============================] - 35s 493ms/step - loss: 1.4377 - accuracy: 0.3652 - val_loss: 1.2734 - val_accuracy: 0.4743\n","Epoch 2/10\n","68/68 [==============================] - 38s 555ms/step - loss: 1.1749 - accuracy: 0.5092 - val_loss: 1.0934 - val_accuracy: 0.5643\n","Epoch 3/10\n","68/68 [==============================] - 33s 489ms/step - loss: 1.0729 - accuracy: 0.5803 - val_loss: 1.0999 - val_accuracy: 0.5699\n","Epoch 4/10\n","68/68 [==============================] - 37s 552ms/step - loss: 1.0398 - accuracy: 0.5886 - val_loss: 1.0524 - val_accuracy: 0.5938\n","Epoch 5/10\n","68/68 [==============================] - 44s 643ms/step - loss: 0.9978 - accuracy: 0.6090 - val_loss: 0.9843 - val_accuracy: 0.6213\n","Epoch 6/10\n","68/68 [==============================] - 41s 609ms/step - loss: 0.9608 - accuracy: 0.6297 - val_loss: 0.9832 - val_accuracy: 0.6176\n","Epoch 7/10\n","68/68 [==============================] - 50s 727ms/step - loss: 0.9247 - accuracy: 0.6500 - val_loss: 0.9092 - val_accuracy: 0.6397\n","Epoch 8/10\n","68/68 [==============================] - 44s 643ms/step - loss: 0.9082 - accuracy: 0.6593 - val_loss: 0.8851 - val_accuracy: 0.6360\n","Epoch 9/10\n","68/68 [==============================] - 38s 564ms/step - loss: 0.9119 - accuracy: 0.6505 - val_loss: 0.8801 - val_accuracy: 0.6599\n","Epoch 10/10\n","68/68 [==============================] - 35s 515ms/step - loss: 0.8276 - accuracy: 0.6842 - val_loss: 0.8711 - val_accuracy: 0.6801\n"]}],"source":["# train to model\n","from tensorflow.keras.callbacks import EarlyStopping\n","history = model.fit(\n","    train_generator, \n","    steps_per_epoch = train_generator.samples // train_generator.batch_size,\n","    epochs = 10,\n","    validation_data = validation_generator,\n","    validation_steps = validation_generator.samples // validation_generator.batch_size,\n","    callbacks = [early_stopping]\n",")"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["18/18 [==============================] - 4s 200ms/step - loss: 0.8898 - accuracy: 0.6588\n","Validation Loss: 0.89\n","Validation Accuracy: 65.88\n"]}],"source":["val_loss, val_accuracy = model.evaluate(validation_generator)\n","print(f'Validation Loss: {val_loss:.2f}')\n","print(f'Validation Accuracy: {val_accuracy * 100:.2f}')"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["69/69 [==============================] - 15s 212ms/step - loss: 0.7669 - accuracy: 0.7079\n","Validation Loss: 0.77\n","Validation Accuracy: 70.79\n"]}],"source":["loss, accuracy = model.evaluate(train_generator)\n","print(f'Validation Loss: {loss:.2f}')\n","print(f'Validation Accuracy: {accuracy * 100:.2f}')"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# mencoba menambha input baru\n","\n","from tensorflow.keras.preprocessing import image\n","import numpy as np\n","\n","def load_and_preprocessing_image(image_path):\n","    img = image.load_img(image_path, target_size=(150, 150))\n","    img_array = image.img_to_array(img)\n","    img_array = np.expand_dims(img_array, axis=0)\n","    img_array /= 225.0\n","    return img_array"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["# memprediksi kelas gambar baru\n","\n","def predict_image_class(img_path, model, class_indices):\n","    img_array = load_and_preprocessing_image(img_path)\n","    predictions = model.predict(img_array)\n","    predicted_class = np.argmax(predictions, axis=1)\n","\n","    # mapping index ke kelas label\n","    class_labels = {v: k for k, v in class_indices.items()}\n","    return class_labels[predicted_class[0]]\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 25ms/step\n","Predicted class : tulip\n"]}],"source":["img_path = \"D:/belajar/Data/Flower/rose kuning.jpg\"\n","predicted_class = predict_image_class(img_path, model, train_generator.class_indices)\n","print(f'Predicted class : {predicted_class}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Ujicobalagi","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":2}
